---
layout: post
title: "Running Open Liberty on Amazon Elastic Container Service with AWS Fargate"
# Do NOT change the categories section
categories: blog
author_picture: https://avatars3.githubusercontent.com/abutch3r
author_github: https://github.com/abutch3r
seo-title: Running Open Liberty on Amazon Elastic Container Service with AWS Fargate - OpenLiberty.io
seo-description: How to run Open Liberty in Amazon Elastic Container Service with AWS Fargate
blog_description: "How to run Open Liberty in Amazon Elastic Container Service with AWS Fargate"
open-graph-image: https://openliberty.io/img/twitter_card.jpg
open-graph-image-alt: Open Liberty Logo
---
= Running Open Liberty on Amazon Elastic Container Service with AWS Fargate
Alex Butcher <https://github.com/abutch3r>
:imagesdir: /
:url-prefix:
:url-about: /

Serverless applications continue to be a hot topic in 2023. With MicroProfile being widely adopted by cloud native applications, many developers want to know:

_How to run MicroProfile applications in Amazon Elastic Container Service in a Serverless manner?_

This post demonstrates how to run a MicroProfile application on Open Liberty in Amazon Elastic Container Service with AWS Fargate.

== What is Serverless?
Serverless is short for serverless computing, which is an execution model in which the cloud provider allocates resources on demand. Serverless enables you to concentrate on your applications without needing to manage servers.

It does not mean no server is running in the background. On the contrary, serverless architecture contains servers, however the cloud provider is responsible for provisioning, maintaining, and scaling the server infrastructure without operator interaction.

Serverless is traditionally seen in the form of functions and is sometimes referred to _Function as a Service_ (FaaS). However, in recent years _Container as a Service_ (CaaS) offerings have become available that have the characteristics to be called serverless offerings.

Some examples of FaaS and CaaS serverless cloud offerings:

*	IBM: IBM Cloud Functions (FaaS) & IBM Cloud Code Engine (CaaS)
*	Amazon Web Services(AWS): AWS Lambda (FaaS) & Amazon Elastic Container Service (CaaS) with AWS Fargate
*	Google Cloud: Google Cloud Functions & Google Cloud Run
*	Microsoft Azure: Azure Functions & Azure Container Apps

This post uses Open Liberty and MicroProfile to run a serverless application in Amazon ECS with AWS Fargate.

=== Amazon Elastic Container Service with AWS Fargate

Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service that helps you easily deploy, manage, and scale containerized applications.

Amazon ECS is built on top of Amazon Elastic Compute Cloud(EC2), with the cloud compute offering providing the servers required to run the application. Amazon EC2 also provides a number of related capabilities such as networking that are used by ECS instances. It is possible to provision and manage the Amazon EC2 Servers the run the ECS cluster workload. As the operator is responsible for managinge the underlying compute resources such as provisioning the servers, Amazon ECS is by itself not a serverless offering.

AWS Fargate is a technology that can be used with Amazon ECS to run containers without having to manage any underlying Amazon EC2 servers or clusters. By removing server resource management capabilities for Amazon ECS, AWS Fargate handles the provisioning, scaling and management of the underlying infrastructure. This allows Amazon ECS with AWS Fargate to provide CaaS capabilities in a serverless manner.

=== MicroProfile

Many developers use https://microprofile.io[MicroProfile] specifications for configuring, securing, and observing cloud native applications. MicroProfile offers a set of standard APIs for cloud-native applications that free your applications from vendor lock-in. Open Liberty is the leading implementation for MicroProfile specifications.

As we enter the serverless era, it is very important to get MicroProfile applications running in a serverless environment. Luckily, it is very straightforward to get these applications running in Container as a Service offerings.

== Deploying an Open Liberty Container to Amazon ECS

=== Pre-requisites
Before you start, at a minimum you will need the following prerequisites:

* An https://aws.amazon.com/[AWS] Administrator account*
* An Amazon ECS Cluster with AWS Fargate

If you are planning on building and containerizing the MicroProfile application and publishing the image for use with Amazon ECS, these are the additional prerequisites required:

* https://www.docker.com/[Docker]
* https://git-scm.com/book/en/v2/Getting-Started-The-Command-Line[Git CLI]
* https://maven.apache.org/[Maven]
* An externally accessible container registry that can host your images such as Amazon Elastic Container Registry(Amazon ECR) or Dockerhub.

=== Getting Started

If you already have experience with Open Liberty and MicroProfile it is recommended you go straight to <<Creating your Amazon ECS Cluster>> as for this blog, there is a publicly accessible image hosted on IBM Cloud Container Registry (ICCR) that can be used.

If you don't have experience with Open Liberty and MicroProfile or wish to understand more about the application used in this blog follow the steps in <<Create your MicroProfile application>> and <<Uploading the container to a registry>>.

=== Create your MicroProfile application
In this post, we use https://openliberty.io/guides/getting-started.html[Open Liberty Getting Started guide] to get a simple MicroProfile application that is well suited to run in a serverless environment.

Follow the guide up to and including https://openliberty.io/guides/getting-started.html#running-the-application-in-a-docker-container[Running the application in a Docker container]

=== Uploading the container to a registry
If you wish to use the image created in <<Create your MicroProfile application>> then it needs to be uploaded to a internet accessible container registry.

Amazon ECS supports a range of container registries outlined in their https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html#container_definition_image[Image Registry requirements].

Depending on your choice of registry follow the steps provided by it for uploading your image.

As this blog requires an AWS account, you are most likely to have access to Amazon ECR to act as your registry. To upload the image use the https://docs.aws.amazon.com/AmazonECR/latest/userguide/getting-started-console.html[Get Started] instructions for Amazon ECR.

=== Creating your Task Definition
Amazon ECS runs either Services or Jobs that use https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html[Task Definitions] to define the runtime configuration for the task. The task definition includes some of the following properties:

* Container Image URL
* CPU & Memory
* Port Mappings
* Environment variables
* Compatibilities

The values of these properties cannot be overridden by the service or job definition that execute the task definition. For example, if the same task definition was used in Development and Production deployments, then they would use the same amount of CPU. As such it is recommended that separate task definitions are defined for each environment.

The instructions below will use a publicly accessible Open Liberty getting started guide container image. If you are using your own container, substitute `icr.io/appcafe/open-liberty/samples/getting-started` with the path for your image.

It is possible to create the Task Definition using the UI or by uploading JSON, for this blog we will upload JSON.

==== Creating Task definition using JSON
To apply the below example task definition to create a new Task definition follow Step 2. in https://docs.aws.amazon.com/AmazonECS/latest/developerguide/getting-started-fargate.html[Getting started with the console using Linux containers on AWS Fargate].

Before applying, ensure that the `logConfiguration.options.awslogs-region` setting matches the region you are planning on deploying into.

Example Open Liberty Amazon ECS Task Definition:

[source]
----
{
    "family": "ol-getting-started-blog",
    "containerDefinitions": [
        {
            "name": "open-liberty-getting-started",
            "image": "icr.io/appcafe/open-liberty/samples/getting-started",
            "cpu": 512,
            "memory": 1024,
            "portMappings": [
                {
                    "name": "liberty-getting-started-80-tcp",
                    "containerPort": 80,
                    "hostPort": 80,
                    "protocol": "tcp",
                    "appProtocol": "http"
                },
                {
                    "name": "liberty-getting-started-443-tcp",
                    "containerPort": 443,
                    "hostPort": 443,
                    "protocol": "tcp",
                    "appProtocol": "http"
                }
            ],
            "essential": true,
            "environment": [
                {
                    "name": "default.http.port",
                    "value": "80"
                },
                {
                    "name": "default.https.port",
                    "value": "443"
                }
            ],
            "environmentFiles": [],
            "mountPoints": [],
            "volumesFrom": [],
            "logConfiguration": {
                "logDriver": "awslogs",
                "options": {
                    "awslogs-create-group": "true",
                    "awslogs-group": "/ecs/ol-getting-started-demo",
                    "awslogs-region": "us-east-1",
                    "awslogs-stream-prefix": "ecs"
                }
            }
        }
    ],
    "executionRoleArn": "",
    "networkMode": "awsvpc",
    "requiresCompatibilities": [
        "FARGATE"
    ],
    "cpu": "512",
    "memory": "1024",
    "runtimePlatform": {
        "cpuArchitecture": "X86_64",
        "operatingSystemFamily": "LINUX"
    }
}
----

The example above contains a few details that should be explained.

By default Open Liberty uses ports 9080 and 9443 for HTTP and HTTPS traffic respectively. The server.xml used by the server on start up looks for two variables `default.http.port` and `default.https.port` port to allow for the ports to be overridden. By setting these to 80 and 443 respectively we can then expose these externally on these ports.


If you are not an admin you will get errors within the application stating that Open Liberty cannot open ports 80 or 443 within the container. This is an environment limitation that cannot be resolved within Amazon ECS, the workaround is to use non-reserved ports such as 9080 and 9443.

If you wish to deploy the container with ports 9080 and 9443 and expose externally on different ports such as 80 and 443, you will need to create all the network artifacts such as target group and load balancer before you create the service.

For CPU and Memory, given the deployed application is simple and we won't be putting it under any heavy load, 0.5 CPU and 1GB of memory is sufficient.

Before creating the task, update the `"awslogs-region"` value to match the region that you are deploying into.

A full list of task definition parameters can be found https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html[here].

As we now have our Task definition, we need to create a service that uses the definition.

=== Creating the Service
For the purposes of this blog we will create an instance that uses HTTP.

The Amazon ECS supports two types of runtime definitions, Service and Tasks. Tasks define batch type workloads and typically don't have any external input while running, while Services are suited to web applications. As such Services are used for this blog.

.To Create the Service
. Go to the Amazon ECS Service
. Go to `Clusters`
. Select the Cluster you created earlier
. Under the Services Tab, click `Create`
. Under `Environment`
.. Update Compute Options from `Capacity provider strategy` to `Launch Type`
.. Ensure Launch type is `Fargate`
. Under `Deployment Configuration`
.. For Family, set to the Task Definition created earlier
.. Provide the service a name e.g. ol-getting-started-service-1
.. Set the desired count to `0`*
. Under `Networking`
.. Leave VPC and subnets as is
.. Select `Create a new security group`
... Add rules set out in <<_security_group_rules, Security Group rules>>
. Under `Load Balancing`
.. Set `Load balancer type` to `Application Load Balancer`
.. Select `Create a new load balancer`
.. Provide a name
.. Ensure the mapping is to the HTTP port for the Task Definition
.. Select Create a new listener
... Select the Listener for Port 80
.. Select `Create a new target group`
.. Update the `Health check path` to use `/health`**
. Click `Create`

&#42; When first created it is possible
&#42;&#42; the `/health` endpoint is provided by the MicroProfile Health feature and is ideal for reporting health in containerized deployments

[#_security_group_rules]
==== Security Group rules
The default security group rules are deliberately restrictive to prevent unintented access. As such rules need to be provided that allow for the communication on a set of ports

The following rules allow for HTTP and HTTPS traffic on both Open Liberty and Protocol default ports and can all be set when creating the service.

.ECS Security Group Rules
|===
|Type |Protocol |Port Range |Source |Values

|HTTP
|TCP
|80
|Anywhere
|0.0.0.0/0, ::/0

|Custom TCP
|TCP
|9080
|Anywhere
|0.0.0.0/0, ::/0

|HTTPS
|TCP
|443
|Anywhere
|0.0.0.0/0, ::/0

|Custom TCP
|TCP
|9443
|Anywhere
|0.0.0.0/0, ::/0
|===

For HTTP and HTTPs Types, some fields like `Protocol` and `Port Range` will be automatically populated.

The above rules will generate a security policy that looks like this
image::/img/blog/amazon-ec2-security-group-port-mapping.png

=== Starting the Service.
Now that the Service has been created with its required assets and the security group has been updated so that we will be able to communicate with it, we can start it.

- Update the service
- Change the value of `Desired tasks` to `1`
- Click `Save`

This will tell Amazon ECS to start our container.

Once it has reported as running and healthy we can look at accessing it.

=== Accessing the Service
With the service now running we can start to make requests against it.

The first step is to get the DNS name for the Load Balancer. We can get the DNS name for the load balancer either from the load balancer itself or from the target Service.

.Obtaining the DNS name from your Load Balancer
. Go to the EC2 Service
. Select `Load Balancers` under `Load Balancing`
` Copy the address from the `DNS name` column

.Obtaining the DNS name of your Load Balancer from the Service
. Go to your cluster
. Select your Service
. Go to the Networking tab
. Either copy or click `open address`

If you have exposed the service on the non-protocol port, you will need to add the port to the URL.

image::/img/blog/amazon_ecs_host_page.png

=== Monitoring our service

With the Service started, we can start to monitor it using the Amazon ECS tooling and Amazon CloudWatch.

==== CPU and Memory usage

Within the service definition we can see a level of CPU and memory usage

image::/img/blog/amazon_ecs_service_health.png[Amazon ECS Service health ,width=90%,align="center"]

==== Logs

Amazon ECS captures the `STDOUT` and `STDERR` output from the instances and provides them in the `Logs` tab within the Service. If logs are written to file, then you would need to log in to the running container to retrieve them.

Each log line is an individual row within the list that is produced within the tab allowing for easier filtering and searching of events and are recoverable post pod termination.

If you have multiple instances of the container running then all of the messages will appear in the table together, though will state which instance they came from. You can review logs of individual instances by clicking on the links.

=== Scaling your application via auto-scaling policies
Manually scaling is ok for testing, but in production we want the environment to use performance indicators to make scaling decisions for us.

Scaling policies can be applied and adjusted after the Service has been created. The policy that you use should best reflect the expected bottlenecks of your application. If your application handles complex workloads the CPU or Memory. It is possible to define more than one scaling policy per service

The policy allows you to define:

* Number of tasks (instances of your application)
    * Minimum number (>=0 &amp; \<= desired tasks)
    * Maximum number (>=0)
* Scaling metric
** Percentage of CPU
** Percentage of Memory
** Number of ALB Requests over a period of time
* Threshold relative to the metric
* Scale in and out periods

The metrics use CloudWatch data and associated "alarms" to trigger automated scale out actions and reviews them based on the periods set to.

The minimum number of tasks can be set to 0, however as Amazon ECS cannot scale up from 0, then the value in setting the minimum to 0 is limited unless you are completely stopping the service.

For Open Liberty, all 3 scaling metrics can be used. The decision as to which as metric to use relates to the nature of the application that has been deployed on to Open Liberty. If you have requests that are CPU heavy, then CPU based alarms would be the recommendation, however if you have high volume, but low CPU requests then ALB requests* might be a better fit.

ECS Scaling policies are split into 2 alarms:

* Scaling out
* Scaling in

The first alarm is the primary one that we set and AWS will provide a metric definition for scaling in that is matched to the scaling out definition, Though both can be adjusted independently of the Service definition.

The alarms gather CloudWatch data based on their metric over time, this is to try and prevent accidental scaling events of both out and in. If an instance were to experience a short high load period, then when compared to corresponding data points, where we are at typical workload then the alarm is not triggered and we don't spin up unneeded instances. For scaling in, this is the reverse in that we don't ideally want to terminate instances that might be handling workload

Given for this blog, we have given our instances a very small amount of memory and CPU, it is best that we use ALB as our scaling metric as it is either to easy to scale on CPU given we can easily hit high CPU values without any significant workload or to hard to do so based on memory.

To create an ALB request Scaling policy, we shall edit our instance:

. Go to your cluster
. Select your Service
. Select `Update service`
. Set the `Desired tasks` to `1`
. Expand `Service auto scaling`
. Set the minimum to `1`
. Set the maximum to `2`
. Click `+ Add scaling policy`
. Give your policy a name e.g. `mp-sp`
. Set the `ECS service metric` to `ALBRequestCountPerTarget`
. Set the Target value to `2`
. Set `Scale out cooldown period` to `30`
. Set `Scale in cooldown period` to `30`
. Click Update

The target value is set to a very low value so that it is easier to cause a scaling out alarm to trigger and create new instances. This value should be scoped to the requirements of the application and also that the amount of other resources provided are capable of handling that type of workload.

image::../img/blog/amazon_ecs_scaling_policy.png[Amazon ECS scaling policy, width=70%,align="center"]

Having created our policy we can now try to cause the alarm to trigger and cause our service to increase the number of instances available.
As we are looking at requests against the ALB, we just need to invoke our applications URL to generate some traffic.

Given that it requires 3 datapoints above our target in a given period, we just need to invoke

image::/img/blog/amazon_ecs_scaled_instances.png[Amazon ECS scaled out service,width=90%,align="center"]

==== Using CloudWatch Metrics

For further information about Amazon ECS scaling policies you can find additional information https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-autoscaling-targettracking.html?icmpid=docs_ecs_hp-deploy-failure-detection[here].