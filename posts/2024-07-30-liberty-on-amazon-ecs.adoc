---
layout: post
title: "Running Open Liberty on Amazon Elastic Container Service with AWS Fargate"
# Do NOT change the categories section
categories: blog
author_picture: https://avatars3.githubusercontent.com/abutch3r
author_github: https://github.com/abutch3r
seo-title: Running Open Liberty on Amazon Elastic Container Service with AWS Fargate - OpenLiberty.io
seo-description: How to run Open Liberty in Amazon Elastic Container Service with AWS Fargate
blog_description: "How to run Open Liberty in Amazon Elastic Container Service with AWS Fargate"
open-graph-image: https://openliberty.io/img/twitter_card.jpg
open-graph-image-alt: Open Liberty Logo
---
= Running Open Liberty on Amazon Elastic Container Service with AWS Fargate
Alex Butcher <https://github.com/abutch3r>
:imagesdir: /
:url-prefix:
:url-about: /

Serverless applications rapidly scale according to workload but with minimal resource costs through pay per usage models. link:https://aws.amazon.com/ecs/[Amazon Elastic Container Service (Amazon ECS)] with link:https://aws.amazon.com/fargate/[AWS Fargate], a link:https://www.ibm.com/topics/containers-as-a-service[Container as a Service (CaaS)] offering, provides a fully-managed container orchestration service to help you easily deploy, manage, and scale containerized applications in a serverless manner. This post describes how to run serverless MicroProfile applications in Amazon ECS.

== Deploying an Open Liberty Container to Amazon ECS

To run a MicroProfile application on Amazon ECS we need to be able to create AWS resources and have an application to run. For this, you'll need to ensure you have an link:https://aws.amazon.com/[AWS] Administrator account. You'll also need link:https://www.docker.com/[Docker], link:https://git-scm.com/book/en/v2/Getting-Started-The-Command-Line[Git CLI], link:https://maven.apache.org/[Maven], and an externally accessible container registry that can host your images such as link:https://aws.amazon.com/ecr/[Amazon Elastic Container Registry(Amazon ECR)] or link:https://docs.docker.com/docker-hub/[Dockerhub].

If you don't already have a MicroProfile application of your own to use to try these steps out, then you can use the application provided in the 'finish' directory of the Open Liberty Getting Started guide: link:https://github.com/OpenLiberty/guide-getting-started/tree/prod/finish. A publicly accessible copy of the containerized application is available from: `icr.io/appcafe/open-liberty/samples/getting-started`.

=== Uploading the container to a registry
To use this 'getting started' image it needs to be uploaded to a internet accessible container registry. Amazon ECS supports a range of container registries outlined in their https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html#container_definition_image[Image Registry requirements]. As this blog requires an AWS account, you most likely already have access to link:https://aws.amazon.com/ecr/[Amazon ECR] to act as your registry. To upload the image use the https://docs.aws.amazon.com/AmazonECR/latest/userguide/getting-started-console.html[Get Started] instructions for Amazon ECR.

=== Creating your Amazon ECS Cluster
To create your Amazon ECS cluster follow step 1. in https://docs.aws.amazon.com/AmazonECS/latest/developerguide/getting-started-fargate.html[Getting started with the console using Linux containers on AWS Fargate].

=== Creating your Task Definition using JSON
Amazon ECS runs either Services or Jobs that use https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html[Task Definitions] to define the runtime configuration for the task. The task definition includes some of the following properties: Container Image URL, CPU & Memory, Port Mappings, Environment variables, and Compatibilities. A full list of task definition parameters can be found https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html[here].

__Note: The values of these properties cannot be overridden by the service or job definition that execute the task definition. For example, if the same task definition was used in Development and Production deployments, then they would use the same amount of CPU. As such it is recommended that separate task definitions are defined for each environment.__

It is possible to create the Task Definition using the UI or by uploading JSON, but for this blog the JSON is provided. The provided JSON helps to highlight key aspects important when defining task definitions. To apply the below example task definition to create a new Task definition follow Step 2. in https://docs.aws.amazon.com/AmazonECS/latest/developerguide/getting-started-fargate.html[Getting started with the console using Linux containers on AWS Fargate]. Before applying, ensure that the `logConfiguration.options.awslogs-region` setting matches the region you are planning on deploying into.

Example Open Liberty Amazon ECS Task Definition:

[source]
----
{
    "family": "ol-getting-started-blog",
    "containerDefinitions": [
        {
            "name": "open-liberty-getting-started",
            "image": "icr.io/appcafe/open-liberty/samples/getting-started",
            "cpu": 512,
            "memory": 1024,
            "portMappings": [
                {
                    "name": "liberty-getting-started-80-tcp",
                    "containerPort": 80,
                    "hostPort": 80,
                    "protocol": "tcp",
                    "appProtocol": "http"
                },
                {
                    "name": "liberty-getting-started-443-tcp",
                    "containerPort": 443,
                    "hostPort": 443,
                    "protocol": "tcp",
                    "appProtocol": "http"
                }
            ],
            "essential": true,
            "environment": [
                {
                    "name": "default.http.port",
                    "value": "80"
                },
                {
                    "name": "default.https.port",
                    "value": "443"
                }
            ],
            "environmentFiles": [],
            "mountPoints": [],
            "volumesFrom": [],
            "logConfiguration": {
                "logDriver": "awslogs",
                "options": {
                    "awslogs-create-group": "true",
                    "awslogs-group": "/ecs/ol-getting-started-demo",
                    "awslogs-region": "us-east-1",
                    "awslogs-stream-prefix": "ecs"
                }
            }
        }
    ],
    "executionRoleArn": "",
    "networkMode": "awsvpc",
    "requiresCompatibilities": [
        "FARGATE"
    ],
    "cpu": "512",
    "memory": "1024",
    "runtimePlatform": {
        "cpuArchitecture": "X86_64",
        "operatingSystemFamily": "LINUX"
    }
}
----

By default, Open Liberty exposes ports 9080 and 9443 for HTTP and HTTPS traffic respectively. The server.xml exposes two variables: `default.http.port` and `default.https.port` that allow the overriding of the default port values. By setting the environment variables to 80 and 443 respectively the service is exposed on those ports instead.

Now that have a Task definition, the next step needed is to create a Service that uses this definition.

=== Creating the Service
For the purposes of this blog you'll create a Service that uses HTTP.

.To Create the Service
. In the Amazon ECS Service, under `Clusters`, select the Cluster you created earlier
. Under the Services Tab for this cluster, click `Create`
. Under `Environment`
.. Update Compute Options from `Capacity provider strategy` to `Launch Type`
.. Ensure Launch type is `Fargate`
. Under `Deployment Configuration`
.. For Family, set to the Task Definition created earlier
.. Provide the service a name e.g. ol-getting-started-service-1
.. Set the desired count to `0`*
. Under `Networking`
.. Leave VPC and subnets as is
.. Select `Create a new security group`
... Add rules set out in <<_security_group_rules, Security Group rules>>
. Under `Load Balancing`
.. Set `Load balancer type` to `Application Load Balancer`
.. Select `Create a new load balancer`
.. Provide a name
.. Ensure the mapping is to the HTTP port for the Task Definition
.. Select Create a new listener
... Select the Listener for Port 80
.. Select `Create a new target group`
.. Update the `Health check path` to use `/health`**
. Click `Create`

&#42; This prevents the immediate starting of the Service until you are ready
&#42;&#42; the `/health` endpoint is provided by MicroProfile Health and is ideal for reporting health in containerized deployments.

[#_security_group_rules]
==== Security Group rules

Next, you'll need to define some new security rules to allow for HTTP and HTTPS traffic on both Open Liberty and HTTP/S default ports. This is because the default security group rules are deliberately restrictive to prevent unintended exposure, so rules need to be defined that allow for the communication on a set of ports.

.ECS Security Group Rules
|===
|Type |Protocol |Port Range |Source |Values

|HTTP
|TCP
|80
|Anywhere
|0.0.0.0/0, ::/0

|Custom TCP
|TCP
|9080
|Anywhere
|0.0.0.0/0, ::/0

|HTTPS
|TCP
|443
|Anywhere
|0.0.0.0/0, ::/0

|Custom TCP
|TCP
|9443
|Anywhere
|0.0.0.0/0, ::/0
|===

The above rules will generate a security policy that looks like this

image::/img/blog/amazon-ec2-security-group-port-mapping.png[,width=90%,align="center"]

=== Starting the Service.
Now that the Service has been created with its required assets and the security group has been updated so that we will be able to communicate with it, we can start it. To start the service we need to update it by changing the value of `Desired tasks` to `1` - remember to click `Save` when you change this! This will start an instance of our container in ECS. Once it has reported as running and healthy we can look at how to access it.

=== Accessing the Service
With the service now running we can start to make requests against it. The first step for this is to get the DNS name for the Load Balancer, which we can get either from the load balancer itself or from the target Service. In this blog, to keep things simple, we're going to go back to our service and get the DNS name from this.

To obtain the DNS name of your Load Balancer from the Service, you'll once more need to navigate to your cluster and select your Service. Once here, click the `Networking tab` and either copy or click `open address`.

__Note: If you have exposed the service on the non-protocol port, you will need to add the port to the URL.__

image::/img/blog/amazon_ecs_host_page.png[,width=90%,align="center"]

=== Monitoring our service

With the Service started, we can start to monitor it using the Amazon ECS tooling and link:https://aws.amazon.com/cloudwatch/[Amazon CloudWatch]. This monitoring data can help enable effective autoscaling that is such a critical component of serverless applications, enabling more efficient resource usage and lower costs.

For CPU and memory usage, we can access the service definition to see this usage.

image::/img/blog/amazon_ecs_service_health.png[Amazon ECS Service health ,width=90%,align="center"]

=== Scaling your application via auto-scaling policies

To enable autoscaling, we can set up scaling policies that dictate as and when we want the components of our application to be scaled. Scaling policies can be applied and adjusted after the Service has been created. The policy that you use should best reflect the expected bottlenecks of your application. *If your application handles complex workloads the CPU or Memory....* It is possible to define more than one scaling policy per service

The policy allows the number of tasks (instances of your application) to be defined and, enables you to use a variety of scaling metrics and set thresholds for these. These metrics use Amazon CloudWatch data and associated "alarms" to trigger automated scale actions and reviews them based on the periods it is set to. They include:
* Percentage of CPU used
* Percentage of Memory used
* Number of Application Load Balancer (ALB) requests over a period of time

For Open Liberty, all of these 3 scaling metrics can be used. The decision as to which metric to use is dependant on the nature of your application. For example, if you have requests that are CPU heavy, then CPU based alarms would be the recommendation, however if you have high volume, but low CPU requests then ALB requests* might be a better fit.

 ECS Scaling policies are split into 2 alarms:

* Scaling out
* Scaling in

'Sacling out' is the primary alarm that we set and AWS will provide a metric definition for 'scaling in' that is matched to the 'scaling out' definition, Though both can be adjusted independently of the Service definition.

To try and prevent accidental scaling events, the alarms gather Amazon CloudWatch data based on their metric over time. For example, if an instance were to experience just a *short* high load period, then when we compare this to corresponding data points where we are at typical workload, then the alarm is *not* triggered and we don't spin up unneeded instances for just this short spike. On the other hands, for scaling in, this is the reverse - we ideally don't want to terminate instances that might be handling workload.

For this blog, as the application is not CPU or memory intensive, we will use the amount of requests hitting the ALB we receive to trigger scaling events.

To create an ALB request Scaling policy, you can edit the instances scaling policy. To do this, once more revisit your cluster and select your Service. Then select `Update service` and set the `Desired tasks` to `1`. Expand `Service auto scaling` and you'll be presented with a form like the one in the diagram below. In this form:
. Set the minimum number of tasks to `1` and the maximum to `2`
. Click `+ Add scaling policy`
. Give your policy a name e.g. `mp-sp`
. Set the `ECS service metric` to `ALBRequestCountPerTarget`
. Set the `Target value` to `2`
. Set both the `Scale *out* cooldown period` and `Scale *in* cooldown period` to `30`
. Click `Update`

The target value is set to a very low value so that it is easier to cause a scaling out alarm to trigger and create new instances. This value should be scoped to the requirements of the application and also that the amount of other resources provided are capable of handling that type of workload.

image::../img/blog/amazon_ecs_scaling_policy.png[Amazon ECS scaling policy, width=70%,align="center"]

Having created our policy we can now test it by attempting to trigger the alarm and cause our service to increase the number of instances available. As our alarm is focused on requests against the ALB, to test it, we should invoke our applications URL to generate some traffic.

Given our alarm requires 3 datapoints above our target threshold in a given period, we just need to invoke it 3 times in the period being measured. After the trigger has been activated you will see the number of instances scale out, showing that autoscaling is working as expected.

image::/img/blog/amazon_ecs_scaled_instances.png[Amazon ECS scaled out service,width=90%,align="center"]

You have now run and scaled your own MicroProfile Application on Amazon ECS with AWS Fargate!

== Summary:

Through this blog you've gained an understanding of the steps required to take a MicroProfile application running with Open Liberty and run and effectively scale it with Amazon ECS with AWS Fargate. 

== Additional Resources
* https://aws.amazon.com/ecs/[Amazon Elastic Container Service]
* https://aws.amazon.com/fargate/[AWS Fargate]

* https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html[Amazon ECS Task Definitions]
* https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/intro.html[Amazon ECS Best Practices]
* https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html[Amazon Elastic Load Balancing]
* https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html#https-listener-certificates[Create an HTTPS listener for your Application Load Balancer]
* https://aws.permissions.cloud/[AWS Permissions]
* https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-autoscaling-targettracking.html?icmpid=docs_ecs_hp-deploy-failure-detection[Scale your Amazon ECS service using a target metric value].

